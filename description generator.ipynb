{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) What is LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when using RNNs for text, character and word-level models, text is encoded into integers for the model to process. In char-level models, we tokenize each letter into a one-hot vector from the corpus of letters. In word-level models, we tokenize each word into a one-hot vector form the corpus of words. \n",
    "\n",
    "#### One-Hot Vector Encoding Example in a Character-Level Network\n",
    "> Corpus: [‘a’, ‘b’, ‘c’, ‘d, ‘e’, ‘f’, g’, ‘h’, ‘i’] — len(Corpus) = 9 <br>\n",
    " Word: [‘bad’] → [‘b’, ‘a’, ‘d’]\n",
    " b → [0, 1, 0, 0, 0, 0, 0, 0, 0], a → [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], d → [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    " \n",
    " Once we have encoded the text into a vector, it is time to train the model using an LSTM. In this code example, we will use the Keras library built on top of Tensorflow to greatly simplify this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) LSTM Application\n",
    "\n",
    "\n",
    "LSTM networks are capable of modeling sequential / temporal aspects of data and hence have been used widely for text, videos, and time-series. Few applications are language modeling, text classification, machine translation, dialog systems, question-answering, speech recognition, translating videos to natural language, image caption generation, hand writing recognition/generation, natural language generation, anomaly detection, and many more in future..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#make sure to import torch above all of libraries! (otherwise, it gives you errors)\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import timeit\n",
    "import shap\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import graphviz\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback\n",
    "import random\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "description = df.long_description\n",
    "description = ' '.join(map(str, description)).lower()\n",
    "description=description.splitlines() #solving \\n\\n problem \n",
    "description = ' '.join(map(str, description)).lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## clean up\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}','\\n','\\r','©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "for punct in puncts:\n",
    "    description=description.replace(punct,'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the most commercially successful conversational ai companies in the world conversica is building the next generation of our artificial intelligence platform and we are looking for data science interns in the areas of natural language processing natural language generation and deep learning recognized by gartner inc harvard business review etc we are passionate driven selfstarting resourceful innovative collaborative and we get a lot done while having fun if that sounds like you then read on  conversica is seeking talented and passionate data scientists to help us evolve our artificial intelligence machine learning and natural language processing systems and technologies  your responsibilities  as conversicas data scientist you will work with other data scientists and engineers to solve and contribute to the innovative products we are building you will be responsible for developing improving and experimenting with methodologies and algorithms to support these efforts using machin'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " description[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(description)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  8098854\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(description)\n",
    "print (\"Total Characters: \", n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1619757 \n",
      "\n",
      "['one of the most commercially successful conversational ai companies in', 'f the most commercially successful conversational ai companies in the ', ' most commercially successful conversational ai companies in the world', ' commercially successful conversational ai companies in the world conv', 'ercially successful conversational ai companies in the world conversic', 'lly successful conversational ai companies in the world conversica is ', 'uccessful conversational ai companies in the world conversica is build', 'sful conversational ai companies in the world conversica is building t', 'conversational ai companies in the world conversica is building the ne', 'rsational ai companies in the world conversica is building the next ge'] \n",
      "\n",
      "[' ', 'w', ' ', 'e', 'a', 'b', 'i', 'h', 'x', 'n']\n"
     ]
    }
   ],
   "source": [
    "maxlen = 70\n",
    "step = 5\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(description) - maxlen, step):\n",
    "    sentences.append(description[i: i + maxlen])\n",
    "    next_chars.append(description[i + maxlen])\n",
    "print('Number of sequences:', len(sentences), \"\\n\")\n",
    "\n",
    "print(sentences[:10], \"\\n\")\n",
    "print(next_chars[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked for specified epochs. Prints generated text.\n",
    "    # Using epoch+1 to be consistent with the training epochs printed by Keras\n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(description) - maxlen - 1)\n",
    "        for diversity in [0.4]:\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = description[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(700):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "            \n",
    "generate_text = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 1448s - loss: 1.7025\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.4\n",
      "----- Generating with seed: \" career can ultimately take you we empower you to do great work in a c\"\n",
      " career can ultimately take you we empower you to do great work in a company opportunity to control and and statistics and a data engineering products with a sql strategics status and individual and and able the specific or interned service and products to enal and the needs to and a distriction and the employee of data and data and the related with opportunity experience with data and develop and and a production machine learning and create with a complex strong strategic products and and stateholofich content and and status of considere who develops and market including of internal statistics status of data and and analytics and and machine learning products and development of and strong operations and and and machine learning and data analysis and work and \n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.70248, saving model to weights.hdf5\n",
      "Epoch 2/5\n",
      " - 1446s - loss: 1.2594\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.4\n",
      "----- Generating with seed: \"at virgin pulse we have so much more than a strong supportive company \"\n",
      "at virgin pulse we have so much more than a strong supportive company and the complex processes and computer science and relationships in the companies the data scientist the successful company of the foundation of analyze in the internal and machine learning and united in the consumer solutions in the providing analysis experience working with company and analysis to analysis and resources and computer science or a statistics of the complex processes and interpresing information and computer science services and analysis and complex the providing the enable and accommodation of the company and technologies and using statistical or relational communication skills and responsibilities that well work in the business bus network and resolutions in realtime and te\n",
      "\n",
      "Epoch 00002: loss improved from 1.70248 to 1.25944, saving model to weights.hdf5\n",
      "Epoch 3/5\n",
      " - 1443s - loss: 1.1639\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.4\n",
      "----- Generating with seed: \"tate laws as well as applicable local ordinances including but not lim\"\n",
      "tate laws as well as applicable local ordinances including but not limited data analyst in the area and other data analysis and analytics and content to ensure data and demands and experience in and engineering and data science and development models and the strategy and work on a company and solutions to communicate and company the company manage and projects and data scientists strong analytics and data scientists and competitive machine learning and results and of the unique of the data sets to contributions to meet the continuous and or related to the findings to production to also content of the armination of the applications and processing and production and community of the company with the world with the position required and communication skills and p\n",
      "\n",
      "Epoch 00003: loss improved from 1.25944 to 1.16391, saving model to weights.hdf5\n",
      "Epoch 4/5\n",
      " - 1448s - loss: 1.1184\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.4\n",
      "----- Generating with seed: \"dels svms boosting neural networks ensemble methods and applied multiv\"\n",
      "dels svms boosting neural networks ensemble methods and applied multivariate compensation in the world with product design of the methodology deep learning engineer in proficiency in a data scientist and machine learning process skills and environment will be in a highly integration and statistical and internal status sexual orientation statistical analysis and responsibilities to improve a successful computer science or equivalent with an equal opportunity employer and experience with cloud services and communication skills and services in the development of experience with customers with a results and experience with compelling presentation skills and relationships and all problem solve products and experience in a company and information products and servic\n",
      "\n",
      "Epoch 00004: loss improved from 1.16391 to 1.11839, saving model to weights.hdf5\n",
      "Epoch 5/5\n"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# fit model using our gpu\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=5,\n",
    "              verbose=2,\n",
    "              callbacks=[generate_text, checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
